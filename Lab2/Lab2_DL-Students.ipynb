{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 1.5 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Group name:</b> Sara Giammusso, Samuel Pierre\n",
    " \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Generate your final report (export as HTML) and upload it on the submission website http://bigfoot-m1.eurecom.fr/teachingsub/login (using your deeplearnXX/password). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed and submitted by May 30th 2018 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%. Can you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks,  **LeNet-5**, to go to more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell below to load the MNIST data that comes with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "epsilon = 1e-10 # this is a parameter you will use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example\n",
    "**y=softmax(Wx+b)** seen in the first lab. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the TensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.288088709\n",
      "Epoch:  02   =====> Loss= 0.732281670\n",
      "Epoch:  03   =====> Loss= 0.600392412\n",
      "Epoch:  04   =====> Loss= 0.536762394\n",
      "Epoch:  05   =====> Loss= 0.497826366\n",
      "Epoch:  06   =====> Loss= 0.470822294\n",
      "Epoch:  07   =====> Loss= 0.451210828\n",
      "Epoch:  08   =====> Loss= 0.435753634\n",
      "Epoch:  09   =====> Loss= 0.423504825\n",
      "Epoch:  10   =====> Loss= 0.413171643\n",
      "Epoch:  11   =====> Loss= 0.404602712\n",
      "Epoch:  12   =====> Loss= 0.396665183\n",
      "Epoch:  13   =====> Loss= 0.390432234\n",
      "Epoch:  14   =====> Loss= 0.384589661\n",
      "Epoch:  15   =====> Loss= 0.379198474\n",
      "Epoch:  16   =====> Loss= 0.374669660\n",
      "Epoch:  17   =====> Loss= 0.370596089\n",
      "Epoch:  18   =====> Loss= 0.366341053\n",
      "Epoch:  19   =====> Loss= 0.363085027\n",
      "Epoch:  20   =====> Loss= 0.359674313\n",
      "Epoch:  21   =====> Loss= 0.356840514\n",
      "Epoch:  22   =====> Loss= 0.353889638\n",
      "Epoch:  23   =====> Loss= 0.351291800\n",
      "Epoch:  24   =====> Loss= 0.348949190\n",
      "Epoch:  25   =====> Loss= 0.346355069\n",
      "Epoch:  26   =====> Loss= 0.344331550\n",
      "Epoch:  27   =====> Loss= 0.342307088\n",
      "Epoch:  28   =====> Loss= 0.340233262\n",
      "Epoch:  29   =====> Loss= 0.338329601\n",
      "Epoch:  30   =====> Loss= 0.336859738\n",
      "Epoch:  31   =====> Loss= 0.335102431\n",
      "Epoch:  32   =====> Loss= 0.333367937\n",
      "Epoch:  33   =====> Loss= 0.332122592\n",
      "Epoch:  34   =====> Loss= 0.330594900\n",
      "Epoch:  35   =====> Loss= 0.329148665\n",
      "Epoch:  36   =====> Loss= 0.327847015\n",
      "Epoch:  37   =====> Loss= 0.326450446\n",
      "Epoch:  38   =====> Loss= 0.325458886\n",
      "Epoch:  39   =====> Loss= 0.324121157\n",
      "Epoch:  40   =====> Loss= 0.323234588\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9159\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/ex1'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=lab_2/log_files/\"**\n",
    "- Click on \"Tensorboard web interface\" in Zoe  \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now more familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet-5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions  for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride, padding_):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model(data, transfer=\"ReLU\", keep_prob=1.):    \n",
    "    \n",
    "    transferFuncs = {\"sigmoid\" : tf.sigmoid, \"ReLU\": tf.nn.relu}\n",
    "        \n",
    "    #first convolutional layer\n",
    "    W_conv1 = weight_variable([5, 5, 1, 6]) ## [filter_width, filter_height, depth_image_in, depth_image_out]\n",
    "    b_conv1 = bias_variable([6])\n",
    "    h_conv1 = transferFuncs[transfer](conv2d(data, W_conv1, 1, 'SAME') + b_conv1)\n",
    "    pool1 = tf.nn.pool(h_conv1, [2,2], \"MAX\", 'VALID', strides=[2,2])\n",
    "    \n",
    "    #second convolutional layer\n",
    "    W_conv2 = weight_variable([5, 5, 6, 16])\n",
    "    b_conv2 = bias_variable([16])\n",
    "    h_conv2 = transferFuncs[transfer](conv2d(pool1, W_conv2, 1, 'VALID') + b_conv2)\n",
    "    pool2 = tf.nn.pool(h_conv2, [2,2], \"MAX\", 'VALID', strides=[2,2])\n",
    "    \n",
    "    #first fully connected layer\n",
    "    s = pool2.get_shape().as_list()\n",
    "    flattened_length = s[1] * s[2] * s[3]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, flattened_length])\n",
    "    W_fc1 = weight_variable([1*5*5*16, 120])\n",
    "    b_fc1 = bias_variable([120])\n",
    "    h_fc1 = transferFuncs[transfer](tf.matmul(pool2_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    #second fully connected layer\n",
    "    W_fc2 = weight_variable([120, 84])\n",
    "    b_fc2 = bias_variable([84])\n",
    "    h_fc2 = transferFuncs[transfer](tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "    \n",
    "    #third fully connected layer\n",
    "    W_fc3 = weight_variable([84, 10])\n",
    "    b_fc3 = bias_variable([10])\n",
    "    h_fc3 = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "    \n",
    "    return h_fc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  61706\n"
     ]
    }
   ],
   "source": [
    "# first conv\n",
    "pconv1 = 5*5*1*6 # filter_height * filter_width * channels_in * num_feature_maps\n",
    "# second conv\n",
    "pconv2 = 5*5*6*16 # filter_height * filter_width * channels_in * num_feature_maps\n",
    "# first fcl\n",
    "pfcl1 = 5*5*16*120 # fcl_input_size * fcl_output_size\n",
    "# second fcl\n",
    "pfcl2 = 84*120 # fcl_input_size * fcl_output_size\n",
    "# third fcl\n",
    "pfcl3 = 84*10 # fcl_input_size * fcl_output_size\n",
    "# all the biases\n",
    "pbias = 6 + 16 + 120 + 84 + 10 \n",
    "\n",
    "total = pbias + pfcl1 + pfcl2 + pfcl3 + pconv2 + pconv1\n",
    "print('Total number of parameters: ', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Fucntion: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # reset the default graph before defining a new model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "\n",
    "logs_path = 'log_files/ex2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, y):\n",
    "    correct = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    return tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def train(learning_rate, training_epochs, batch_size, display_step = 1, \\\n",
    "          logs_path='log_files/', optFunction=\"SGD\", verbose=True, transfer=\"ReLU\", keep_probability= 1.0):\n",
    "    \n",
    "    optFunctions = {\"SGD\":tf.train.GradientDescentOptimizer, \"Adam\":tf.train.AdamOptimizer}\n",
    "    \n",
    "    # Erase previous graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28, 1], name='InputData')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Construct model\n",
    "    with tf.name_scope('Model'):\n",
    "        pred = LeNet5_Model(x, transfer=transfer)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    with tf.name_scope('Loss'):\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "\n",
    "    with tf.name_scope(optFunction):\n",
    "        if transfer is \"sigmoid\":\n",
    "            optimizer = optFunctions[optFunction](learning_rate).minimize(cost)\n",
    "        else:\n",
    "            opt = optFunctions[optFunction](learning_rate)\n",
    "            gvs = opt.compute_gradients(cost)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "            optimizer = opt.apply_gradients(capped_gvs)\n",
    "\n",
    "    # Evaluate model\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = evaluate(pred, y)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"Loss\", cost)\n",
    "    # Create a summary to monitor accuracy tensor\n",
    "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    x_val, y_val = mnist.validation.images.reshape(-1, 28, 28, 1), mnist.validation.labels\n",
    "    x_test, y_test = mnist.test.images.reshape(-1, 28, 28, 1), mnist.test.labels\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        if verbose is True:\n",
    "            print(\"Start Training!\")\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "        # Training cycle\n",
    "        start_time = time.time()\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle = True)\n",
    "                batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                         feed_dict={x: batch_xs, y: batch_ys, keep_prob: keep_probability})\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "            \n",
    "            val_acc = accuracy.eval({x: x_val, y:y_val, keep_prob:keep_probability})\n",
    "            \n",
    "            \n",
    "            if verbose is True and (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \\\n",
    "                      \"  =====> Loss=\", \"{:.9f}\".format(avg_cost), \\\n",
    "                      \" Validation accuracy=\", val_acc)\n",
    "            if val_acc >= 0.99:\n",
    "                if verbose is True:\n",
    "                    print(\"Validation Accuracy over 99.0%% reached after %d epochs\" %(epoch+1))\n",
    "        \n",
    "        elapsed_time = time.time() - start_time \n",
    "        saver.save(sess, 'Models/model_' + str(learning_rate) + '_' + str(batch_size) + '_' + optFunction)\n",
    "        if verbose is True:\n",
    "            print(\"Training Finished!\")\n",
    "            # Test model\n",
    "            # Calculate accuracy\n",
    "            print(\"Test accuracy:\", accuracy.eval({x: x_test, y:y_test, keep_prob: keep_probability}))\n",
    "            print(\"Elapsed time: \", elapsed_time, \"sec\")\n",
    "        \n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 2.307532670  Validation accuracy= 0.1632\n",
      "Epoch:  02   =====> Loss= 2.271728622  Validation accuracy= 0.221\n",
      "Epoch:  03   =====> Loss= 2.232151651  Validation accuracy= 0.3728\n",
      "Epoch:  04   =====> Loss= 2.163331968  Validation accuracy= 0.446\n",
      "Epoch:  05   =====> Loss= 2.019186282  Validation accuracy= 0.5414\n",
      "Epoch:  06   =====> Loss= 1.697722448  Validation accuracy= 0.6544\n",
      "Epoch:  07   =====> Loss= 1.206095699  Validation accuracy= 0.7522\n",
      "Epoch:  08   =====> Loss= 0.826219373  Validation accuracy= 0.8086\n",
      "Epoch:  09   =====> Loss= 0.634631187  Validation accuracy= 0.8436\n",
      "Epoch:  10   =====> Loss= 0.520621628  Validation accuracy= 0.8676\n",
      "Epoch:  11   =====> Loss= 0.456320666  Validation accuracy= 0.8874\n",
      "Epoch:  12   =====> Loss= 0.403626301  Validation accuracy= 0.8994\n",
      "Epoch:  13   =====> Loss= 0.367017818  Validation accuracy= 0.9062\n",
      "Epoch:  14   =====> Loss= 0.340801885  Validation accuracy= 0.9136\n",
      "Epoch:  15   =====> Loss= 0.316848219  Validation accuracy= 0.918\n",
      "Epoch:  16   =====> Loss= 0.298322453  Validation accuracy= 0.9234\n",
      "Epoch:  17   =====> Loss= 0.283225182  Validation accuracy= 0.9276\n",
      "Epoch:  18   =====> Loss= 0.270770618  Validation accuracy= 0.9308\n",
      "Epoch:  19   =====> Loss= 0.260363104  Validation accuracy= 0.9346\n",
      "Epoch:  20   =====> Loss= 0.246600396  Validation accuracy= 0.9354\n",
      "Epoch:  21   =====> Loss= 0.240255123  Validation accuracy= 0.937\n",
      "Epoch:  22   =====> Loss= 0.229533199  Validation accuracy= 0.9394\n",
      "Epoch:  23   =====> Loss= 0.225151163  Validation accuracy= 0.941\n",
      "Epoch:  24   =====> Loss= 0.213860937  Validation accuracy= 0.942\n",
      "Epoch:  25   =====> Loss= 0.210373405  Validation accuracy= 0.9456\n",
      "Epoch:  26   =====> Loss= 0.202366667  Validation accuracy= 0.9468\n",
      "Epoch:  27   =====> Loss= 0.198284120  Validation accuracy= 0.9482\n",
      "Epoch:  28   =====> Loss= 0.192679624  Validation accuracy= 0.95\n",
      "Epoch:  29   =====> Loss= 0.189048684  Validation accuracy= 0.9512\n",
      "Epoch:  30   =====> Loss= 0.180947510  Validation accuracy= 0.9524\n",
      "Epoch:  31   =====> Loss= 0.177356102  Validation accuracy= 0.955\n",
      "Epoch:  32   =====> Loss= 0.174801082  Validation accuracy= 0.9572\n",
      "Epoch:  33   =====> Loss= 0.172426109  Validation accuracy= 0.9578\n",
      "Epoch:  34   =====> Loss= 0.165372786  Validation accuracy= 0.958\n",
      "Epoch:  35   =====> Loss= 0.163509329  Validation accuracy= 0.9586\n",
      "Epoch:  36   =====> Loss= 0.159717225  Validation accuracy= 0.96\n",
      "Epoch:  37   =====> Loss= 0.158715805  Validation accuracy= 0.962\n",
      "Epoch:  38   =====> Loss= 0.152434150  Validation accuracy= 0.9618\n",
      "Epoch:  39   =====> Loss= 0.149891468  Validation accuracy= 0.9624\n",
      "Epoch:  40   =====> Loss= 0.148953789  Validation accuracy= 0.9644\n",
      "Training Finished!\n",
      "Test accuracy: 0.961\n",
      "Elapsed time:  678.1924319267273 sec\n"
     ]
    }
   ],
   "source": [
    "elapsed_time_1 = train(learning_rate, training_epochs, batch_size,logs_path=logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./MNIST_figures/loss1.png\" style=\"width:80%\">\n",
    "  \n",
    "<img src=\"./MNIST_figures/accuracy1.png\" style=\"width:80%\">\n",
    "<center><span>Figure 2: Loss and Accuracy for LeNet5 with SGD</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>\n",
    "\n",
    "- Retrain your network with AdamOptimizer and then fill the table above:\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |       95.41%       |       98.24%        |       \n",
    "| Training Time        |       11 min       |       11 min        |  \n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Answer 2.2.1 </strong><br>\n",
    "The optimizer that gives the best accuracy on test data is AdamOptimizer. Indeed, with AdamOptimizer we reach 99% in just 8 epochs, while with SGD we never reach 99%, neither in 40 epochs. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 0.329192257  Validation accuracy= 0.9692\n",
      "Epoch:  02   =====> Loss= 0.086178050  Validation accuracy= 0.9798\n",
      "Epoch:  03   =====> Loss= 0.058729552  Validation accuracy= 0.986\n",
      "Epoch:  04   =====> Loss= 0.045659533  Validation accuracy= 0.9866\n",
      "Epoch:  05   =====> Loss= 0.039085610  Validation accuracy= 0.9876\n",
      "Epoch:  06   =====> Loss= 0.030334682  Validation accuracy= 0.9896\n",
      "Epoch:  07   =====> Loss= 0.026520149  Validation accuracy= 0.9876\n",
      "Epoch:  08   =====> Loss= 0.024417924  Validation accuracy= 0.9904\n",
      "Validation Accuracy over 99.0% reached after 8 epochs\n",
      "Epoch:  09   =====> Loss= 0.019348163  Validation accuracy= 0.9888\n",
      "Epoch:  10   =====> Loss= 0.017935880  Validation accuracy= 0.9886\n",
      "Epoch:  11   =====> Loss= 0.015129979  Validation accuracy= 0.9904\n",
      "Validation Accuracy over 99.0% reached after 11 epochs\n",
      "Epoch:  12   =====> Loss= 0.013902304  Validation accuracy= 0.9908\n",
      "Validation Accuracy over 99.0% reached after 12 epochs\n",
      "Epoch:  13   =====> Loss= 0.010909236  Validation accuracy= 0.9872\n",
      "Epoch:  14   =====> Loss= 0.011273074  Validation accuracy= 0.9914\n",
      "Validation Accuracy over 99.0% reached after 14 epochs\n",
      "Epoch:  15   =====> Loss= 0.011291960  Validation accuracy= 0.9908\n",
      "Validation Accuracy over 99.0% reached after 15 epochs\n",
      "Epoch:  16   =====> Loss= 0.008571214  Validation accuracy= 0.9884\n",
      "Epoch:  17   =====> Loss= 0.008057100  Validation accuracy= 0.9904\n",
      "Validation Accuracy over 99.0% reached after 17 epochs\n",
      "Epoch:  18   =====> Loss= 0.007319304  Validation accuracy= 0.9892\n",
      "Epoch:  19   =====> Loss= 0.007865470  Validation accuracy= 0.992\n",
      "Validation Accuracy over 99.0% reached after 19 epochs\n",
      "Epoch:  20   =====> Loss= 0.006898928  Validation accuracy= 0.9904\n",
      "Validation Accuracy over 99.0% reached after 20 epochs\n",
      "Epoch:  21   =====> Loss= 0.005178228  Validation accuracy= 0.991\n",
      "Validation Accuracy over 99.0% reached after 21 epochs\n",
      "Epoch:  22   =====> Loss= 0.006556299  Validation accuracy= 0.9916\n",
      "Validation Accuracy over 99.0% reached after 22 epochs\n",
      "Epoch:  23   =====> Loss= 0.004776817  Validation accuracy= 0.986\n",
      "Epoch:  24   =====> Loss= 0.007557891  Validation accuracy= 0.9902\n",
      "Validation Accuracy over 99.0% reached after 24 epochs\n",
      "Epoch:  25   =====> Loss= 0.004684811  Validation accuracy= 0.9902\n",
      "Validation Accuracy over 99.0% reached after 25 epochs\n",
      "Epoch:  26   =====> Loss= 0.003676942  Validation accuracy= 0.9908\n",
      "Validation Accuracy over 99.0% reached after 26 epochs\n",
      "Epoch:  27   =====> Loss= 0.004100946  Validation accuracy= 0.992\n",
      "Validation Accuracy over 99.0% reached after 27 epochs\n",
      "Epoch:  28   =====> Loss= 0.004666811  Validation accuracy= 0.99\n",
      "Validation Accuracy over 99.0% reached after 28 epochs\n",
      "Epoch:  29   =====> Loss= 0.003964738  Validation accuracy= 0.9894\n",
      "Epoch:  30   =====> Loss= 0.003884294  Validation accuracy= 0.9886\n",
      "Epoch:  31   =====> Loss= 0.005495541  Validation accuracy= 0.9904\n",
      "Validation Accuracy over 99.0% reached after 31 epochs\n",
      "Epoch:  32   =====> Loss= 0.004645035  Validation accuracy= 0.9914\n",
      "Validation Accuracy over 99.0% reached after 32 epochs\n",
      "Epoch:  33   =====> Loss= 0.003083152  Validation accuracy= 0.9924\n",
      "Validation Accuracy over 99.0% reached after 33 epochs\n",
      "Epoch:  34   =====> Loss= 0.003586007  Validation accuracy= 0.9914\n",
      "Validation Accuracy over 99.0% reached after 34 epochs\n",
      "Epoch:  35   =====> Loss= 0.004575062  Validation accuracy= 0.9898\n",
      "Epoch:  36   =====> Loss= 0.003155469  Validation accuracy= 0.9914\n",
      "Validation Accuracy over 99.0% reached after 36 epochs\n",
      "Epoch:  37   =====> Loss= 0.002653381  Validation accuracy= 0.989\n",
      "Epoch:  38   =====> Loss= 0.003175316  Validation accuracy= 0.9924\n",
      "Validation Accuracy over 99.0% reached after 38 epochs\n",
      "Epoch:  39   =====> Loss= 0.004858406  Validation accuracy= 0.9914\n",
      "Validation Accuracy over 99.0% reached after 39 epochs\n",
      "Epoch:  40   =====> Loss= 0.001364608  Validation accuracy= 0.9922\n",
      "Validation Accuracy over 99.0% reached after 40 epochs\n",
      "Training Finished!\n",
      "Test accuracy: 0.9894\n",
      "Elapsed time:  675.4184231758118 sec\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "logs_path = 'log_files/ex3/'\n",
    "elapsed_time_2 = train(learning_rate, training_epochs, batch_size, optFunction=\"Adam\",logs_path=logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./MNIST_figures/loss2.png\" style=\"width:80%\">\n",
    "  \n",
    "<img src=\"./MNIST_figures/accuracy2.png\" style=\"width:80%\">\n",
    "<center><span>Figure 3: Loss and Accuracy for LeNet5 with Adam</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2</b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use tf.nn.dropout for that purpose. What accuracy do you achieve on testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 0.369031923  Validation accuracy= 0.9708\n",
      "Epoch:  02   =====> Loss= 0.093144498  Validation accuracy= 0.9816\n",
      "Epoch:  03   =====> Loss= 0.067406275  Validation accuracy= 0.9832\n",
      "Epoch:  04   =====> Loss= 0.050735684  Validation accuracy= 0.9864\n",
      "Epoch:  05   =====> Loss= 0.042091775  Validation accuracy= 0.9866\n",
      "Epoch:  06   =====> Loss= 0.035156979  Validation accuracy= 0.986\n",
      "Epoch:  07   =====> Loss= 0.030891549  Validation accuracy= 0.9886\n",
      "Epoch:  08   =====> Loss= 0.027827556  Validation accuracy= 0.99\n",
      "Validation Accuracy over 99.0% reached after 8 epochs\n",
      "Epoch:  09   =====> Loss= 0.021516716  Validation accuracy= 0.9908\n",
      "Validation Accuracy over 99.0% reached after 9 epochs\n",
      "Epoch:  10   =====> Loss= 0.020065733  Validation accuracy= 0.9888\n",
      "Epoch:  11   =====> Loss= 0.017868793  Validation accuracy= 0.9862\n",
      "Epoch:  12   =====> Loss= 0.013813605  Validation accuracy= 0.9874\n",
      "Epoch:  13   =====> Loss= 0.012400943  Validation accuracy= 0.9888\n",
      "Epoch:  14   =====> Loss= 0.014231061  Validation accuracy= 0.9892\n",
      "Epoch:  15   =====> Loss= 0.011037812  Validation accuracy= 0.992\n",
      "Validation Accuracy over 99.0% reached after 15 epochs\n",
      "Epoch:  16   =====> Loss= 0.010104288  Validation accuracy= 0.9878\n",
      "Epoch:  17   =====> Loss= 0.008786007  Validation accuracy= 0.989\n",
      "Epoch:  18   =====> Loss= 0.008717570  Validation accuracy= 0.9872\n",
      "Epoch:  19   =====> Loss= 0.007595177  Validation accuracy= 0.9888\n",
      "Epoch:  20   =====> Loss= 0.007942765  Validation accuracy= 0.9882\n",
      "Epoch:  21   =====> Loss= 0.005538650  Validation accuracy= 0.9896\n",
      "Epoch:  22   =====> Loss= 0.009248968  Validation accuracy= 0.9904\n",
      "Validation Accuracy over 99.0% reached after 22 epochs\n",
      "Epoch:  23   =====> Loss= 0.005282890  Validation accuracy= 0.992\n",
      "Validation Accuracy over 99.0% reached after 23 epochs\n",
      "Epoch:  24   =====> Loss= 0.005072487  Validation accuracy= 0.9904\n",
      "Validation Accuracy over 99.0% reached after 24 epochs\n",
      "Epoch:  25   =====> Loss= 0.004095910  Validation accuracy= 0.9904\n",
      "Validation Accuracy over 99.0% reached after 25 epochs\n",
      "Epoch:  26   =====> Loss= 0.007863344  Validation accuracy= 0.9862\n",
      "Epoch:  27   =====> Loss= 0.005607009  Validation accuracy= 0.9912\n",
      "Validation Accuracy over 99.0% reached after 27 epochs\n",
      "Epoch:  28   =====> Loss= 0.001937612  Validation accuracy= 0.9894\n",
      "Epoch:  29   =====> Loss= 0.005462932  Validation accuracy= 0.9912\n",
      "Validation Accuracy over 99.0% reached after 29 epochs\n",
      "Epoch:  30   =====> Loss= 0.006510452  Validation accuracy= 0.9912\n",
      "Validation Accuracy over 99.0% reached after 30 epochs\n",
      "Epoch:  31   =====> Loss= 0.003423018  Validation accuracy= 0.9894\n",
      "Epoch:  32   =====> Loss= 0.002010077  Validation accuracy= 0.9918\n",
      "Validation Accuracy over 99.0% reached after 32 epochs\n",
      "Epoch:  33   =====> Loss= 0.004637112  Validation accuracy= 0.9904\n",
      "Validation Accuracy over 99.0% reached after 33 epochs\n",
      "Epoch:  34   =====> Loss= 0.006575180  Validation accuracy= 0.9916\n",
      "Validation Accuracy over 99.0% reached after 34 epochs\n",
      "Epoch:  35   =====> Loss= 0.005202931  Validation accuracy= 0.9904\n",
      "Validation Accuracy over 99.0% reached after 35 epochs\n",
      "Epoch:  36   =====> Loss= 0.001175785  Validation accuracy= 0.9928\n",
      "Validation Accuracy over 99.0% reached after 36 epochs\n",
      "Epoch:  37   =====> Loss= 0.001929070  Validation accuracy= 0.992\n",
      "Validation Accuracy over 99.0% reached after 37 epochs\n",
      "Epoch:  38   =====> Loss= 0.005023757  Validation accuracy= 0.9906\n",
      "Validation Accuracy over 99.0% reached after 38 epochs\n",
      "Epoch:  39   =====> Loss= 0.004508169  Validation accuracy= 0.991\n",
      "Validation Accuracy over 99.0% reached after 39 epochs\n",
      "Epoch:  40   =====> Loss= 0.004138340  Validation accuracy= 0.9918\n",
      "Validation Accuracy over 99.0% reached after 40 epochs\n",
      "Training Finished!\n",
      "Test accuracy: 0.9924\n",
      "Elapsed time:  694.7537033557892 sec\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "logs_path = 'log_files/ex4/'\n",
    "elapsed_time_3 = train(learning_rate, training_epochs, batch_size, optFunction=\"Adam\", logs_path=logs_path, keep_probability= 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"./MNIST_figures/loss3.png\" style=\"width:80%\">\n",
    "<img src=\"./MNIST_figures/accuracy3.png\" style=\"width:80%\">\n",
    "<center><span>Figure 3: Loss and Accuracy for LeNet5 with Adam and Dropout</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Answer 2.2.2 </strong><br>\n",
    "Accuracy achieved on testing data: 99.24%.<br>\n",
    "<p>We reach a better accuracy than the previous experiment!</p>\n",
    "<p>Dropout refers to ignoring units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random. By “ignoring”, we mean these units are not considered during a particular forward or backward pass.</p>\n",
    "<p>Dropout is a good technique to prevent overfitting. Indeed, a fully connected layer occupies most of the parameters, and hence, neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to over-fitting of training data.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"./MNIST_figures/loss_ensemble.png\" style=\"width:80%\">\n",
    "<img src=\"./MNIST_figures/acc_ensemble.png\" style=\"width:80%\">\n",
    "<center><span>Figure 4: Loss and Accuracy final comparison</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Final comparison</strong>\n",
    "<p>Figure 4 shows a comparison between accuracy and loss of the three models.<br>\n",
    "The blue curve represents the model built using SGD, the red line the model built using AdamOptimizer and the turquoise curve the model built using AdamOptimizer and the Dropout.</p>\n",
    "\n",
    "<p>SGD loss decreases more much slower than Adam loss. At the same time Adam accuracy is already very high at the very beginning. We don't notice particular differences between Adam model with and without dropout in these plots, even if the final test accuracy is 0.9924 using dropout and 0.9894 without it.</p>\n",
    "\n",
    "<p>We can conclude that Adam is a better optimizer than SGD.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
